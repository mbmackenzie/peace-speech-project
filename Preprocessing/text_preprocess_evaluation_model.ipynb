{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_preprocess_evaluation_model",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOFtSYn5XjS2GFa3SfjBO/+"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0314e94eb78f4eb8a2e70a21734cda66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d4ed628e56f44e338aa981d02810b1c6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_34edfa566d0e4d1594f42afe012a996e",
              "IPY_MODEL_a39c03cd113149178a840fc34629676d"
            ]
          }
        },
        "d4ed628e56f44e338aa981d02810b1c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "34edfa566d0e4d1594f42afe012a996e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7e266213f1554182b13250390d8e3966",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ebf828f61c8b425089daea623e261979"
          }
        },
        "a39c03cd113149178a840fc34629676d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_61cdfea200ee42d684208a0b56c39a1a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [00:00&lt;00:00, 3.33kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_762a962e744d4474bb277493860591c2"
          }
        },
        "7e266213f1554182b13250390d8e3966": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ebf828f61c8b425089daea623e261979": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "61cdfea200ee42d684208a0b56c39a1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "762a962e744d4474bb277493860591c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dc8038e86c154439a956f2a2c50a3bb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f81807cae71a4691aa84d15e14a691f9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6455c614cd4f4b32a9cd7cbabbb70de5",
              "IPY_MODEL_96c6002fad0149a5b09f9fe3d1fe0a00"
            ]
          }
        },
        "f81807cae71a4691aa84d15e14a691f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6455c614cd4f4b32a9cd7cbabbb70de5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4324bed31b0247a382efd3e7175217e3",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e1a5bd9aa0a24d159eceedaf04709976"
          }
        },
        "96c6002fad0149a5b09f9fe3d1fe0a00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f14e5a6a4acd4420bd615c3b6abceade",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 440M/440M [00:52&lt;00:00, 8.38MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3f6d6ecfcffd4949bea209587914d6f9"
          }
        },
        "4324bed31b0247a382efd3e7175217e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e1a5bd9aa0a24d159eceedaf04709976": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f14e5a6a4acd4420bd615c3b6abceade": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3f6d6ecfcffd4949bea209587914d6f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZhYJVjDFBlf"
      },
      "source": [
        "# About the Notebook\n",
        "\n",
        "This notebook is about creating a classification model to evaluate the performance of text preprocess technique. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0HA2dtKFCFS"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2xX5nx7V_3h",
        "outputId": "3fca19bd-a731-49e5-e974-d1caab29de57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import torch\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')\n",
        "\n",
        "#To confirm that we are using GPU for the training later\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n",
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla V100-SXM2-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhnY7CIfzA4f",
        "outputId": "7baa1e59-84ad-4669-f5e1-af40dd673eea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import re\n",
        "import nltk\n",
        "#from sentence_transformers import SentenceTransformer\n",
        "import datetime\n",
        "from scipy.spatial.distance import cosine\n",
        "from sklearn.model_selection import KFold\n",
        "import random\n",
        "import time\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, random_split,DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertForTokenClassification, AdamW, BertConfig, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "from transformers import RobertaTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
        "model = AutoModel.from_pretrained(\"allenai/longformer-base-4096\")\n",
        "tokenizer.model_max_length = model.config.max_position_embeddings\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZd8Iea7f_Cf"
      },
      "source": [
        "text_file_path = '/content/text.txt'\n",
        "source_file_path = '/content/now-samples-sources.txt'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ik0Kt2C1qMEa"
      },
      "source": [
        "## Import data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPN1pltbPuD0"
      },
      "source": [
        "sources = pd.read_csv(source_file_path, sep=\"\\t{1,2}\", encoding=\"ISO-8859-1\", skiprows=2, engine='python')\n",
        "# rename columns\n",
        "sources.columns = [\"id\", \"n_words\", \"date\", \"country\", \"website\", \"url\", \"title\"]\n",
        "\n",
        "# date column -> pandas.DateTime\"\n",
        "sources[\"date\"] = pd.to_datetime(sources[\"date\"], format=\"%y-%m-%d\")\n",
        "\n",
        "pd.set_option('display.max_colwidth', 40 )\n",
        "\n",
        "with open(text_file_path, \"r\") as f:\n",
        "    text = pd.DataFrame(\n",
        "        [re.search(\"(\\d+)\\s(.*)\", l[2:]).groups() for l in f.readlines() if l.startswith(\"@@\")],\n",
        "        columns=[\"id\", \"text\"]     )\n",
        "    f.close()\n",
        "\n",
        "# id should be an integer\n",
        "text[\"id\"] = text[\"id\"].astype(int)\n",
        "#text['text'] = text.text.str.lower()\n",
        "\n",
        "reports = sources.merge(text, on=\"id\", how=\"outer\")\n",
        "reports[\"year\"] = reports[\"date\"].dt.strftime(\"%Y\")\n",
        "reports = reports[reports['text'].notna()]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDaK5e_Si1kL"
      },
      "source": [
        "## Basic Preprocessing\n",
        "\n",
        "The below function will clean the html codes and some special characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKvVSTY4i8PD"
      },
      "source": [
        "import html\n",
        "def preprocess(doc):\n",
        "  doc = html.unescape(doc)\n",
        "  doc = doc.replace('{', '')\n",
        "  doc = doc.replace('}', '')\n",
        "  doc = doc.replace(\"\\n\", '')\n",
        "  doc = doc.rstrip(\"\\n\") #remove empty lines\n",
        "  doc = doc.replace(\"@ @ @ @ @ @ @ @ @ @ \", '')\n",
        "  doc = doc.replace(\" @\", '')\n",
        "  doc = doc.replace(\" '\", \"'\")\n",
        "  doc = doc.replace(\"\\\"\", \"\")\n",
        "  doc = doc.replace(\",\", \"\")\n",
        "  doc = doc.replace(\"(\", \"\")\n",
        "  doc = doc.replace(\")\", \"\")\n",
        "  doc = doc.replace(\" <p>\", \".\")\n",
        "  doc = doc.replace(\" <h>\", \".\")\n",
        "  doc = doc.replace(\"<p>\", \"\")\n",
        "  doc = doc.replace(\"<h>\", \"\")\n",
        "  doc = doc.replace('<', '')\n",
        "  doc = doc.replace('>', '')\n",
        "  doc = doc.replace(\":\", \"\")\n",
        "  #doc = doc.replace(\"?\", \".\")\n",
        "  #doc = doc.replace(\"!\", \".\")\n",
        "  doc = doc.replace(\" ?\", \"?\")\n",
        "  doc = doc.replace(\" !\", \"!\")\n",
        "  doc = doc.replace(r\"\\.\\s[\\.\\s]+\", \". \") #converting . . to .\n",
        "  doc = doc.replace(r\"\\.+\", \".\") #converting ... to .\n",
        "  doc = doc.replace(\"--\", \"\") \n",
        "  doc = doc.replace(\"-\", \" \")\n",
        "  doc = doc.replace(\" +\", \" \")\n",
        "  doc = doc.replace(\" n't\", \"n't\")\n",
        "  doc = doc.replace(\" ..\", \".\")\n",
        "  doc = doc.replace(\"..\", \".\")\n",
        "  doc = doc.replace(\"  \", \" \")\n",
        "  doc = doc.replace(\" .\", \".\")\n",
        "  return doc"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kfQolyais_h"
      },
      "source": [
        "## Create Random sampled data for manual cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRHZGHnLiwvk"
      },
      "source": [
        "#Randomly Sample 100 reports for the cleaning\n",
        "reports['text'] = reports['text'].apply(preprocess)\n",
        "reports = reports.dropna()\n",
        "reports['text'] = reports['text'].apply(nltk.sent_tokenize)\n",
        "rand_index = np.random.randint(0, high = len(reports), size = 100)\n",
        "report_to_clean = reports.iloc[rand_index, :]\n",
        "report_to_clean.to_csv('report_to_clean.csv')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Tlq3tpviwuW"
      },
      "source": [
        "#Import the cleaned data (cleaning done by human)\n",
        "report_cleaned = pd.read_csv('/content/clean_report.csv', encoding=\"ISO-8859-1\" )\n",
        "report_cleaned = report_cleaned.iloc[:,3:]\n",
        "noise_data_base = report_cleaned['text']\n",
        "clean_report = report_cleaned.copy()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Tvtih6jvZXc",
        "outputId": "2d177354-6bbe-4d34-be27-fa738c557cbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        }
      },
      "source": [
        "#EDA Part\n",
        "#from easy_data_augmentation import *#Reference: https://github.com/jasonwei20/eda_nlp\n",
        "'''\n",
        "With the EDA, we will create total of 500 data with label 'Clean' (400 generation from EDA)\n",
        "'''\n",
        "from easy_data_augmentation import *\n",
        "\n",
        "df_dict = {\"id\": [], 'n_words':[], 'date':[], \"website\": [], \"url\":[], \"title\": [],\"text\":[], \"year\":[],}\n",
        "for i in range(len(report_cleaned)):\n",
        "  text = report_cleaned['text'][i]\n",
        "  for j in range(4):\n",
        "    tok_sent = nltk.sent_tokenize(text)\n",
        "    for k in range(len(tok_sent)):\n",
        "      alpha = 0.1 + j * 0.1\n",
        "      try:\n",
        "        tok_sent[k] = eda_for_one_sentence(tok_sent[k], alpha_sr = alpha, alpha_ri = alpha, alpha_rs = alpha, p_rd = alpha)\n",
        "      except:\n",
        "        pass\n",
        "    df_dict['id'].append(report_cleaned['id'][i])\n",
        "    df_dict['url'].append(report_cleaned['url'][i])\n",
        "    df_dict['title'].append(report_cleaned['title'][i])\n",
        "    df_dict['website'].append(report_cleaned['website'][i])\n",
        "    df_dict['text'].append(' '.join(tok_sent))\n",
        "    df_dict['date'].append(report_cleaned['date'][i])\n",
        "    df_dict['n_words'].append(report_cleaned['n_words'][i])\n",
        "    df_dict['year'].append(report_cleaned['year'][i])\n",
        "    ret_text = ' '.join(tok_sent)\n",
        "\n",
        "\n",
        "df_append = pd.DataFrame.from_dict(df_dict)\n",
        "df_append"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>n_words</th>\n",
              "      <th>date</th>\n",
              "      <th>website</th>\n",
              "      <th>url</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3071244</td>\n",
              "      <td>761</td>\n",
              "      <td>2015-02-27</td>\n",
              "      <td>The Guardian</td>\n",
              "      <td>http://www.theguardian.com/sport/201...</td>\n",
              "      <td>UCI moves to revoke licence of Vince...</td>\n",
              "      <td>in an unprecedented move inward the ...</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3071244</td>\n",
              "      <td>761</td>\n",
              "      <td>2015-02-27</td>\n",
              "      <td>The Guardian</td>\n",
              "      <td>http://www.theguardian.com/sport/201...</td>\n",
              "      <td>UCI moves to revoke licence of Vince...</td>\n",
              "      <td>in licence an act human beings gover...</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3071244</td>\n",
              "      <td>761</td>\n",
              "      <td>2015-02-27</td>\n",
              "      <td>The Guardian</td>\n",
              "      <td>http://www.theguardian.com/sport/201...</td>\n",
              "      <td>UCI moves to revoke licence of Vince...</td>\n",
              "      <td>put forward hold unprecedented move ...</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3071244</td>\n",
              "      <td>761</td>\n",
              "      <td>2015-02-27</td>\n",
              "      <td>The Guardian</td>\n",
              "      <td>http://www.theguardian.com/sport/201...</td>\n",
              "      <td>UCI moves to revoke licence of Vince...</td>\n",
              "      <td>in jacques incite of duty for bespea...</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4621241</td>\n",
              "      <td>263</td>\n",
              "      <td>2014-02-24</td>\n",
              "      <td>The Star Online</td>\n",
              "      <td>http://www.thestar.com.my/news/natio...</td>\n",
              "      <td>Cyber-bullying reports up 55.6% in 2013</td>\n",
              "      <td>cyber intimidation  reports 6 up 201...</td>\n",
              "      <td>2014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>395</th>\n",
              "      <td>1721243</td>\n",
              "      <td>495</td>\n",
              "      <td>2010-11-20</td>\n",
              "      <td>Telegraph.co.uk</td>\n",
              "      <td>http://www.telegraph.co.uk/finance/f...</td>\n",
              "      <td>IMF's Dominique Strauss-Kahn wants f...</td>\n",
              "      <td>fund likely make do study torso on i...</td>\n",
              "      <td>2010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>396</th>\n",
              "      <td>1681243</td>\n",
              "      <td>836</td>\n",
              "      <td>2010-10-21</td>\n",
              "      <td>Toronto Star</td>\n",
              "      <td>http://www.thestar.com/news/gta/2010...</td>\n",
              "      <td>James: Mississauga needs watchdog 'g...</td>\n",
              "      <td>mccallion hasnt campaigned hazel thi...</td>\n",
              "      <td>2010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>397</th>\n",
              "      <td>1681243</td>\n",
              "      <td>836</td>\n",
              "      <td>2010-10-21</td>\n",
              "      <td>Toronto Star</td>\n",
              "      <td>http://www.thestar.com/news/gta/2010...</td>\n",
              "      <td>James: Mississauga needs watchdog 'g...</td>\n",
              "      <td>pomaderris this mccallion hasnt camp...</td>\n",
              "      <td>2010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>398</th>\n",
              "      <td>1681243</td>\n",
              "      <td>836</td>\n",
              "      <td>2010-10-21</td>\n",
              "      <td>Toronto Star</td>\n",
              "      <td>http://www.thestar.com/news/gta/2010...</td>\n",
              "      <td>James: Mississauga needs watchdog 'g...</td>\n",
              "      <td>filbert if always crusade this decad...</td>\n",
              "      <td>2010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>399</th>\n",
              "      <td>1681243</td>\n",
              "      <td>836</td>\n",
              "      <td>2010-10-21</td>\n",
              "      <td>Toronto Star</td>\n",
              "      <td>http://www.thestar.com/news/gta/2010...</td>\n",
              "      <td>James: Mississauga needs watchdog 'g...</td>\n",
              "      <td>hard eer inward hazelnut difficult h...</td>\n",
              "      <td>2010</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>400 rows Ã— 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          id  n_words  ...                                     text  year\n",
              "0    3071244      761  ...  in an unprecedented move inward the ...  2015\n",
              "1    3071244      761  ...  in licence an act human beings gover...  2015\n",
              "2    3071244      761  ...  put forward hold unprecedented move ...  2015\n",
              "3    3071244      761  ...  in jacques incite of duty for bespea...  2015\n",
              "4    4621241      263  ...  cyber intimidation  reports 6 up 201...  2014\n",
              "..       ...      ...  ...                                      ...   ...\n",
              "395  1721243      495  ...  fund likely make do study torso on i...  2010\n",
              "396  1681243      836  ...  mccallion hasnt campaigned hazel thi...  2010\n",
              "397  1681243      836  ...  pomaderris this mccallion hasnt camp...  2010\n",
              "398  1681243      836  ...  filbert if always crusade this decad...  2010\n",
              "399  1681243      836  ...  hard eer inward hazelnut difficult h...  2010\n",
              "\n",
              "[400 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BrNAQJ4niHz",
        "outputId": "9bfc0e22-5de6-4588-9ef5-0e9793e1ae25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "report_cleaned = report_cleaned.append(df_append)\n",
        "report_cleaned.index = np.arange(0, len(report_cleaned))\n",
        "report_cleaned"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>n_words</th>\n",
              "      <th>date</th>\n",
              "      <th>country</th>\n",
              "      <th>website</th>\n",
              "      <th>url</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3071244</td>\n",
              "      <td>761</td>\n",
              "      <td>2015-02-27</td>\n",
              "      <td>GB</td>\n",
              "      <td>The Guardian</td>\n",
              "      <td>http://www.theguardian.com/sport/201...</td>\n",
              "      <td>UCI moves to revoke licence of Vince...</td>\n",
              "      <td>In an unprecedented move the world ...</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4621241</td>\n",
              "      <td>263</td>\n",
              "      <td>2014-02-24</td>\n",
              "      <td>MY</td>\n",
              "      <td>The Star Online</td>\n",
              "      <td>http://www.thestar.com.my/news/natio...</td>\n",
              "      <td>Cyber-bullying reports up 55.6% in 2013</td>\n",
              "      <td>Cyber bullying reports up 55.6% in 2...</td>\n",
              "      <td>2014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4041244</td>\n",
              "      <td>200</td>\n",
              "      <td>2011-03-08</td>\n",
              "      <td>GB</td>\n",
              "      <td>Express.co.uk</td>\n",
              "      <td>http://www.express.co.uk/celebrity-n...</td>\n",
              "      <td>Klum's underwear gift was part of pl...</td>\n",
              "      <td>Klum's underwear gift was part of p...</td>\n",
              "      <td>2011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8901244</td>\n",
              "      <td>412</td>\n",
              "      <td>2016-05-18</td>\n",
              "      <td>ZA</td>\n",
              "      <td>Citizen</td>\n",
              "      <td>http://www.citizen.co.za/1121347/inj...</td>\n",
              "      <td>Injured Jobodwana retains focus on O...</td>\n",
              "      <td>Injured Jobodwana retains focus on ...</td>\n",
              "      <td>2016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2781242</td>\n",
              "      <td>382</td>\n",
              "      <td>2014-09-15</td>\n",
              "      <td>ZA</td>\n",
              "      <td>SuperSport</td>\n",
              "      <td>http://www.supersport.com/cricket/do...</td>\n",
              "      <td>Morkel out of action for at least te...</td>\n",
              "      <td>Cricket Domestic Cricket. Morkel ou...</td>\n",
              "      <td>2014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>495</th>\n",
              "      <td>1721243</td>\n",
              "      <td>495</td>\n",
              "      <td>2010-11-20</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Telegraph.co.uk</td>\n",
              "      <td>http://www.telegraph.co.uk/finance/f...</td>\n",
              "      <td>IMF's Dominique Strauss-Kahn wants f...</td>\n",
              "      <td>fund likely make do study torso on i...</td>\n",
              "      <td>2010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>496</th>\n",
              "      <td>1681243</td>\n",
              "      <td>836</td>\n",
              "      <td>2010-10-21</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Toronto Star</td>\n",
              "      <td>http://www.thestar.com/news/gta/2010...</td>\n",
              "      <td>James: Mississauga needs watchdog 'g...</td>\n",
              "      <td>mccallion hasnt campaigned hazel thi...</td>\n",
              "      <td>2010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>1681243</td>\n",
              "      <td>836</td>\n",
              "      <td>2010-10-21</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Toronto Star</td>\n",
              "      <td>http://www.thestar.com/news/gta/2010...</td>\n",
              "      <td>James: Mississauga needs watchdog 'g...</td>\n",
              "      <td>pomaderris this mccallion hasnt camp...</td>\n",
              "      <td>2010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498</th>\n",
              "      <td>1681243</td>\n",
              "      <td>836</td>\n",
              "      <td>2010-10-21</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Toronto Star</td>\n",
              "      <td>http://www.thestar.com/news/gta/2010...</td>\n",
              "      <td>James: Mississauga needs watchdog 'g...</td>\n",
              "      <td>filbert if always crusade this decad...</td>\n",
              "      <td>2010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499</th>\n",
              "      <td>1681243</td>\n",
              "      <td>836</td>\n",
              "      <td>2010-10-21</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Toronto Star</td>\n",
              "      <td>http://www.thestar.com/news/gta/2010...</td>\n",
              "      <td>James: Mississauga needs watchdog 'g...</td>\n",
              "      <td>hard eer inward hazelnut difficult h...</td>\n",
              "      <td>2010</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500 rows Ã— 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          id  n_words  ...                                     text  year\n",
              "0    3071244      761  ...   In an unprecedented move the world ...  2015\n",
              "1    4621241      263  ...  Cyber bullying reports up 55.6% in 2...  2014\n",
              "2    4041244      200  ...   Klum's underwear gift was part of p...  2011\n",
              "3    8901244      412  ...   Injured Jobodwana retains focus on ...  2016\n",
              "4    2781242      382  ...   Cricket Domestic Cricket. Morkel ou...  2014\n",
              "..       ...      ...  ...                                      ...   ...\n",
              "495  1721243      495  ...  fund likely make do study torso on i...  2010\n",
              "496  1681243      836  ...  mccallion hasnt campaigned hazel thi...  2010\n",
              "497  1681243      836  ...  pomaderris this mccallion hasnt camp...  2010\n",
              "498  1681243      836  ...  filbert if always crusade this decad...  2010\n",
              "499  1681243      836  ...  hard eer inward hazelnut difficult h...  2010\n",
              "\n",
              "[500 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6R-if6kD1qs"
      },
      "source": [
        "from transformers import *\n",
        "\n",
        "#Tokenize the text (sent_tok), and create text that will be less than 512 tokens\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5ccUoN4Eovk"
      },
      "source": [
        "test = report_cleaned.copy()\n",
        "text_for_preprocess = []\n",
        "for i in range(len(test)):\n",
        "  text = test['text'][i]\n",
        "  sent_tok = nltk.sent_tokenize(text)\n",
        "  text_divided = []\n",
        "  tok_length = 0\n",
        "  text_temp = ''\n",
        "  for sent in sent_tok:\n",
        "    length_tokens = len(tokenizer.tokenize(sent))\n",
        "    tok_length += length_tokens\n",
        "    if tok_length < 400:\n",
        "      text_temp += ' ' + sent\n",
        "    else:\n",
        "      text_divided.append(text_temp)\n",
        "      text_temp = sent\n",
        "      tok_length = length_tokens\n",
        "  if text_temp != '':\n",
        "    text_divided.append(text_temp)\n",
        "  text_for_preprocess.append(text_divided)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmJvD9EMU-Q5"
      },
      "source": [
        "flat_list_clean = [item for sublist in text_for_preprocess for item in sublist]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxhHrthj1WvR",
        "outputId": "84a7c501-c777-4c7c-f636-0666bb8076a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "length = []\n",
        "for i in range(len(flat_list_clean)):\n",
        "  length.append(len(tokenizer.tokenize(flat_list_clean[i])))\n",
        "\n",
        "length = pd.DataFrame(length)\n",
        "length.describe()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>938.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>292.801706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>110.532277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>13.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>215.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>352.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>385.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>399.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                0\n",
              "count  938.000000\n",
              "mean   292.801706\n",
              "std    110.532277\n",
              "min     13.000000\n",
              "25%    215.000000\n",
              "50%    352.000000\n",
              "75%    385.750000\n",
              "max    399.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Py0dLV4vyCV"
      },
      "source": [
        "## Add Noise to the text data\n",
        "\n",
        "The Noise data is the sentences that we have deleted. We will going to create the noise data with changing the some part of the sentences as the same method as EDA, and just put them into clean data, then label them as noise data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72TGKpRgvZV7"
      },
      "source": [
        "'''\n",
        "With the Noise, we will create total of 500 data with label 'Noise'\n",
        "'''\n",
        "Noise_example = [\n",
        "                'footballisfun saysMay 30 2012 837 PM.', \n",
        "                 'Baha all comments on this thread about the jags moving are now proven to be moronic.', \n",
        "                 'Fact!', \n",
        "                 'read latest article ; Hahahaha GO jAGS!',\n",
        "                 'Get daily news by email.',\n",
        "                 'Invalid e mailThanks for subscribing!',\n",
        "                 'Could not subscribe try again later.',\n",
        "                 'COMMENT DISCLAIMER Reader comments posted on this Web site are not in any way endorsed by The Standard.', \n",
        "                 'Comments are views by thestandard.ph readers who exercise their right to free position or viewpoint of thestandard.ph.', \n",
        "                 \"While reserving this publication's right to delete comments that are deemed offensive indecent or inconsistent with The Standard editorial standards The Standard may not be held liable for any false information posted by readers in this comments section.\",\n",
        "                 'Add New Comment.', \n",
        "                 '3 Comments.', \n",
        "                 'We believe that many of the attributes you wisely advocate can be learned and mastered.',\n",
        "                 'Perhaps you and your readers may find our related research organizing principles and tools to be of relevance to your important content see the latter two links the last being a related white paper.',\n",
        "                 'Well this is related http **50;0;TOOLONG from the panel discussion Empowered Communities The Art and Science of Building Networks from Nov 20th 2009 at 1000 AM PT at the Monterey Institute of International Studies in Monterey CA.',\n",
        "                 'What can we do in the context of relationships?',\n",
        "                 'Thank you.',\n",
        "                 ' Share this.',\n",
        "                 'From Around the Web.', \n",
        "                 'More From The Times of India.', \n",
        "                 'Recommended By Colombia.', \n",
        "                 'Comments.', \n",
        "                 'Characters Remaining 3000.',\n",
        "                 'OR PROCEED.', \n",
        "                 'FacebookGoogleEmail.', \n",
        "                 'Refrain from posting comments that are obscene defamatory or inflammatory and do not indulge in personal attacks name calling or inciting hatred against any community.', \n",
        "                 'Help us delete comments that do not follow these guidelines by marking them offensive.', \n",
        "                 \"Let's work together to keep the conversation civil.\",\n",
        "                 \"Subscribe us on Youtube.\",\n",
        "                 \"Follow us on Instagram.\",\n",
        "                 \"Follow us on Twitter.\",\n",
        "                 'TNN Jun 2 2015 09.01 PM IST.',\n",
        "                 'Aditya Singh/AFP/Getty Images.',\n",
        "                 'Getty Images.',\n",
        "                 'Sponsored.',\n",
        "                 'Advert.',\n",
        "                 \"Only Buchanan's commentaries in your email.\", \n",
        "                 \"BONUS By signing up for Pat Buchanan's weekly alerts you will also be signed up for news and special offers from WND via email.\", \n",
        "                 'Name*.', \n",
        "                 'FirstLast.', \n",
        "                 'Email*.', \n",
        "                 'Where we will email your daily updates.', \n",
        "                 'Postal code*.', \n",
        "                 'A valid zip code or postal code is required.', \n",
        "                 \"Click the button below to sign up for Pat Buchanan's commentaries by email and keep up to date with special offers from WND.\", \n",
        "                 'You may change your email preferences at any time.',\n",
        "                 'We encourage but we ask you to follow our guidelines for respecting community standards.', \n",
        "                 'Personal attacks inappropriate language and off topic comments may be removed and comment privileges revoked per our Terms of Use.', \n",
        "                 'Please see our FAQ if you have questions or concerns about using Facebook to comment.',\n",
        "                 'Read more.',\n",
        "                 'Disclaimer.', \n",
        "                 'You understand and agree that no content published on the Site constitutes a recommendation that any particular security portfolio of securities transaction or investment strategy is suitable or advisable for any specific person.', \n",
        "                 'You further understand that none of the information providers or their affiliates will advise you personally concerning the nature potential advisability value or suitability of any particular stock share security portfolio of securities transaction investment strategy or other matter.', \n",
        "                 'We openly disclose that we and our contributors may have interests in investments and/or providers of services referred to within the website and that we receive remuneration from certain of the companies referred to on this website.'\n",
        "  ]\n",
        "eda_noise_sen = []\n",
        "\n",
        "for sen in Noise_example:\n",
        "  for i in range(6):\n",
        "    alpha = 0.1 + i * 0.1\n",
        "    aug_sent = eda_for_one_sentence(sen, alpha_sr = alpha, alpha_ri = alpha, alpha_rs = alpha, p_rd = alpha)\n",
        "    eda_noise_sen.append(aug_sent)\n",
        "eda_noise_sen = pd.DataFrame(eda_noise_sen)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XwjWquidB4o",
        "outputId": "500b902c-44ab-4d13-9ed2-ed720bfa2676",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "#Example of EDA sentence\n",
        "eda_noise_sen"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>footballisfun 30 saysmay 2012 837 p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>footballisfun saysmay pm 2012.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>footballisfun xxx 837 pm.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>footballisfun rector saysmay ground ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>postmortem xxx 2012 prove testing sa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>319</th>\n",
              "      <td>we discover of we and our company su...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>320</th>\n",
              "      <td>we openly that along inward contribu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>321</th>\n",
              "      <td>matter to openly make for web inside...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>322</th>\n",
              "      <td>genus crataegus this upward we indis...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>323</th>\n",
              "      <td>cyberspace along we web site our ins...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>324 rows Ã— 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           0\n",
              "0     footballisfun 30 saysmay 2012 837 p...\n",
              "1             footballisfun saysmay pm 2012.\n",
              "2                  footballisfun xxx 837 pm.\n",
              "3    footballisfun rector saysmay ground ...\n",
              "4    postmortem xxx 2012 prove testing sa...\n",
              "..                                       ...\n",
              "319  we discover of we and our company su...\n",
              "320  we openly that along inward contribu...\n",
              "321  matter to openly make for web inside...\n",
              "322  genus crataegus this upward we indis...\n",
              "323  cyberspace along we web site our ins...\n",
              "\n",
              "[324 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0S-CxnL7_NiZ"
      },
      "source": [
        "#Create noise data by inserting the noise text withint the context\n",
        "noise_data = []\n",
        "noise_sentences = eda_noise_sen.iloc[:,0].values\n",
        "for i in range(len(clean_report)):\n",
        "  text = clean_report['text'][i]\n",
        "  for j in range(5):\n",
        "    alpha =  j * 0.1\n",
        "    sent_tok = nltk.sent_tokenize(text)\n",
        "    tok_length = 0\n",
        "    text_temp = ''\n",
        "    text_divided = []\n",
        "    for k in range(len(sent_tok)):\n",
        "      if alpha > 0:\n",
        "        sent_tok[k] = eda_for_one_sentence(sent_tok[k], alpha_sr = alpha, alpha_ri = alpha, alpha_rs = alpha, p_rd = alpha)\n",
        "      length_tokens = len(tokenizer.tokenize(sent_tok[k]))\n",
        "      tok_length += length_tokens\n",
        "      if tok_length < 370:\n",
        "        text_temp += ' ' + sent_tok[k]\n",
        "      else:\n",
        "        subsen_tok = nltk.sent_tokenize(text_temp)\n",
        "        num_sent_to_add = np.random.randint(low = 1, high = 3)\n",
        "        sent_index = np.random.randint(low = 0, high = len(eda_noise_sen), size = num_sent_to_add)\n",
        "        place_to_insert = list(np.random.randint(low = 0, high = len(subsen_tok), size = num_sent_to_add))\n",
        "        place_to_insert.sort()\n",
        "        place_to_insert.reverse()\n",
        "        for index in range(len(sent_index)):\n",
        "          subsen_tok.insert(place_to_insert[index], noise_sentences[index])\n",
        "        text_temp = ' '.join(subsen_tok)\n",
        "        text_divided.append(text_temp)\n",
        "        text_temp = sent_tok[k]\n",
        "        tok_length = length_tokens\n",
        "    if text_temp != '':\n",
        "      text_divided.append(text_temp)\n",
        "    noise_data.append(text_divided)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVkUS5aQBBWz",
        "outputId": "39a29c43-214b-406e-f1f8-f2a25204b395",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "flat_list_noise = [item for sublist in noise_data for item in sublist]\n",
        "length = []\n",
        "for i in range(len(flat_list_noise)):\n",
        "  length.append(len(tokenizer.tokenize(flat_list_noise[i])))\n",
        "\n",
        "length = pd.DataFrame(length)\n",
        "length.describe()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>988.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>287.400810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>106.667809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>9.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>219.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>344.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>375.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>392.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                0\n",
              "count  988.000000\n",
              "mean   287.400810\n",
              "std    106.667809\n",
              "min      9.000000\n",
              "25%    219.000000\n",
              "50%    344.000000\n",
              "75%    375.000000\n",
              "max    392.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWEOJj-GUVcl",
        "outputId": "010f12e6-c6e2-4c10-ed3a-6259321a0478",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print('We have ', len(flat_list_clean), ' number of clean data')\n",
        "print('We have ', len(flat_list_noise), ' number of noise data')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We have  938  number of clean data\n",
            "We have  988  number of noise data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hP3HvaE0U3Qs",
        "outputId": "d13dca22-cb8b-4825-d21f-45c96e1c9f17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "label = [0] * len(flat_list_clean) + [1] * len(flat_list_noise)\n",
        "data = flat_list_clean + flat_list_noise\n",
        "dataset = pd.DataFrame(data, columns = ['data'],)\n",
        "dataset['label'] = label\n",
        "dataset"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>data</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In an unprecedented move the world...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The Italian media alleged in Decembe...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Speaking to reporters at the world t...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Cyber bullying reports up 55.6% in ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Klum's underwear gift was part of ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1921</th>\n",
              "      <td>its a mccallion wallpaper wild fake ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1922</th>\n",
              "      <td>that is gamy nation was equal alread...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1923</th>\n",
              "      <td>effort difficult tree hasnt crusade...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1924</th>\n",
              "      <td>what not real number locution ace ad...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1925</th>\n",
              "      <td>city twelve too $38000 month afterno...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1926 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         data  label\n",
              "0       In an unprecedented move the world...      0\n",
              "1     The Italian media alleged in Decembe...      0\n",
              "2     Speaking to reporters at the world t...      0\n",
              "3      Cyber bullying reports up 55.6% in ...      0\n",
              "4       Klum's underwear gift was part of ...      0\n",
              "...                                       ...    ...\n",
              "1921  its a mccallion wallpaper wild fake ...      1\n",
              "1922  that is gamy nation was equal alread...      1\n",
              "1923   effort difficult tree hasnt crusade...      1\n",
              "1924  what not real number locution ace ad...      1\n",
              "1925  city twelve too $38000 month afterno...      1\n",
              "\n",
              "[1926 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04d5MOZM7qok"
      },
      "source": [
        "## Model for classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAf9R-mWULj8"
      },
      "source": [
        "#===============================================================================\n",
        "# The preprocess code for the BERT\n",
        "# The following code is from https://mccormickml.com/2019/07/22/BERT-fine-tuning/\n",
        "#===============================================================================\n",
        "\n",
        "MAX_LEN = 512\n",
        "\n",
        "\n",
        "def preprocessing_for_bert(data):\n",
        "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
        "    @param    data (np.array): Array of texts to be processed.\n",
        "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
        "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
        "                  tokens should be attended to by the model.\n",
        "    \"\"\"\n",
        "    # Create empty lists to store outputs\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    # For every sentence...\n",
        "    for sent in data:\n",
        "        # `encode_plus` will:\n",
        "        #    (1) Tokenize the sentence\n",
        "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
        "        #    (3) Truncate/Pad sentence to max length\n",
        "        #    (4) Map tokens to their IDs\n",
        "        #    (5) Create attention mask\n",
        "        #    (6) Return a dictionary of outputs\n",
        "        encoded_sent = tokenizer.encode_plus(\n",
        "            sent,  # Preprocess sentence\n",
        "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
        "            max_length=MAX_LEN,                  # Max length to truncate/pad\n",
        "            pad_to_max_length=True,         # Pad sentence to max length\n",
        "            #return_tensors='pt',           # Return PyTorch tensor\n",
        "            return_attention_mask=True,      # Return attention mask\n",
        "            truncation = True\n",
        "            )\n",
        "        \n",
        "        # Add the outputs to the lists\n",
        "        input_ids.append(encoded_sent.get('input_ids'))\n",
        "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
        "\n",
        "    # Convert lists to tensors\n",
        "    input_ids = torch.tensor(input_ids)\n",
        "    attention_masks = torch.tensor(attention_masks)\n",
        "\n",
        "    return input_ids, attention_masks"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4rh6flhUvYt",
        "outputId": "10e94499-380a-45f5-f221-a2bcce2347df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#===============================================================================\n",
        "# Declare the BERT Classifier for later train and test purpose\n",
        "# if one wants to change the model, one can modify the below section\n",
        "#===============================================================================\n",
        "\n",
        "%%time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "\n",
        "# Create the BertClassfier class\n",
        "class BertClassifier(nn.Module):\n",
        "    \"\"\"Bert Model for Classification Tasks.\n",
        "    \"\"\"\n",
        "    def __init__(self, freeze_bert=False, layers_to_freeze = []):\n",
        "        \"\"\"\n",
        "        @param    bert: a BertModel object\n",
        "        @param    classifier: a torch.nn.Module classifier\n",
        "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
        "        \"\"\"\n",
        "        super(BertClassifier, self).__init__()\n",
        "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
        "        D_in, H, D_out = 768, 100, 3\n",
        "\n",
        "        # Instantiate BERT model\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        # Instantiate an one-layer feed-forward classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(D_in, H),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(H, D_out)\n",
        "        )\n",
        "\n",
        "        # Freeze the BERT model\n",
        "        if freeze_bert:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "        else:\n",
        "          if layers_to_freeze != []:\n",
        "            for i in layers_to_freeze:\n",
        "              for param in self.bert.encoder.layer[i].parameters():\n",
        "                param.requires_grad = False\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        \"\"\"\n",
        "        Feed input to BERT and the classifier to compute logits.\n",
        "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
        "                      max_length)\n",
        "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
        "                      information with shape (batch_size, max_length)\n",
        "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
        "                      num_labels)\n",
        "        \"\"\"\n",
        "        # Feed input to BERT\n",
        "        outputs = self.bert(input_ids=input_ids,\n",
        "                            attention_mask=attention_mask)\n",
        "        \n",
        "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
        "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
        "\n",
        "        # Feed input to classifier to compute logits\n",
        "        logits = self.classifier(last_hidden_state_cls)\n",
        "\n",
        "        return logits"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 45 Âµs, sys: 0 ns, total: 45 Âµs\n",
            "Wall time: 56.3 Âµs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3Hg-ZXsVKun"
      },
      "source": [
        "#===============================================================================\n",
        "# Initialize model for the later train purpose\n",
        "#===============================================================================\n",
        "\n",
        "def initialize_model(epochs=4, layers_to_freeze = []):\n",
        "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
        "    \"\"\"\n",
        "    os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "    # Instantiate Bert Classifier\n",
        "    bert_classifier = BertClassifier(freeze_bert=False, layers_to_freeze = layers_to_freeze)\n",
        "\n",
        "    # Tell PyTorch to run the model on GPU\n",
        "    bert_classifier.to(device)\n",
        "\n",
        "    # Create the optimizer\n",
        "    optimizer = AdamW(bert_classifier.parameters(),\n",
        "                      lr=2e-5,    # Default learning rate\n",
        "                      eps=1e-8    # Default epsilon value\n",
        "                      )\n",
        "\n",
        "    # Total number of training steps\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "    # Set up the learning rate scheduler\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                num_warmup_steps=0, # Default value\n",
        "                                                num_training_steps=total_steps)\n",
        "    return bert_classifier, optimizer, scheduler"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0UWoY8_VTDv"
      },
      "source": [
        "#===============================================================================\n",
        "# Train & Evaluate function \n",
        "#===============================================================================\n",
        "\n",
        "import random\n",
        "import time\n",
        "\n",
        "# Specify loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def set_seed(seed_value=42):\n",
        "    \"\"\"Set seed for reproducibility.\n",
        "    \"\"\"\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)\n",
        "\n",
        "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
        "    \"\"\"Train the BertClassifier model.\n",
        "    \"\"\"\n",
        "    # Start training loop\n",
        "    print(\"Start training...\\n\")\n",
        "    for epoch_i in range(epochs):\n",
        "        # =======================================\n",
        "        #               Training\n",
        "        # =======================================\n",
        "        # Print the header of the result table\n",
        "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
        "        print(\"-\"*70)\n",
        "\n",
        "        # Measure the elapsed time of each epoch\n",
        "        t0_epoch, t0_batch = time.time(), time.time()\n",
        "\n",
        "        # Reset tracking variables at the beginning of each epoch\n",
        "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
        "\n",
        "        # Put the model into the training mode\n",
        "        model.train()\n",
        "\n",
        "        # For each batch of training data...\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            batch_counts +=1\n",
        "            # Load batch to GPU\n",
        "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "            # Zero out any previously calculated gradients\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Perform a forward pass. This will return logits.\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "            # Compute loss and accumulate the loss values\n",
        "            loss = loss_fn(logits, b_labels)\n",
        "            batch_loss += loss.item()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Perform a backward pass to calculate gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            # Update parameters and the learning rate\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Print the loss values and time elapsed for every 20 batches\n",
        "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
        "                # Calculate time elapsed for 20 batches\n",
        "                time_elapsed = time.time() - t0_batch\n",
        "\n",
        "                # Print training results\n",
        "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
        "\n",
        "                # Reset batch tracking variables\n",
        "                batch_loss, batch_counts = 0, 0\n",
        "                t0_batch = time.time()\n",
        "\n",
        "        # Calculate the average loss over the entire training data\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "        print(\"-\"*70)\n",
        "        # =======================================\n",
        "        #               Evaluation\n",
        "        # =======================================\n",
        "        if evaluation == True:\n",
        "            # After the completion of each training epoch, measure the model's performance\n",
        "            # on our validation set.\n",
        "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
        "\n",
        "            # Print performance over the entire training data\n",
        "            time_elapsed = time.time() - t0_epoch\n",
        "            \n",
        "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
        "            print(\"-\"*70)\n",
        "        print(\"\\n\")\n",
        "\n",
        "    return float(val_loss), float(val_accuracy)\n",
        "    print(\"Training complete!\")\n",
        "\n",
        "\n",
        "def evaluate(model, val_dataloader):\n",
        "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
        "    on our validation set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    val_accuracy = []\n",
        "    val_loss = []\n",
        "\n",
        "    # For each batch in our validation set...\n",
        "    for batch in val_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_fn(logits, b_labels)\n",
        "        val_loss.append(loss.item())\n",
        "\n",
        "        # Get the predictions\n",
        "        preds = torch.argmax(logits, dim=1).flatten()\n",
        "\n",
        "        # Calculate the accuracy rate\n",
        "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
        "        val_accuracy.append(accuracy)\n",
        "\n",
        "    # Compute the average accuracy and loss over the validation set.\n",
        "    val_loss = np.mean(val_loss)\n",
        "    val_accuracy = np.mean(val_accuracy)\n",
        "\n",
        "    return val_loss, val_accuracy"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FG_A1NRiVVUy",
        "outputId": "49f4feca-4eb1-4430-ba2e-d69f0090f6c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0314e94eb78f4eb8a2e70a21734cda66",
            "d4ed628e56f44e338aa981d02810b1c6",
            "34edfa566d0e4d1594f42afe012a996e",
            "a39c03cd113149178a840fc34629676d",
            "7e266213f1554182b13250390d8e3966",
            "ebf828f61c8b425089daea623e261979",
            "61cdfea200ee42d684208a0b56c39a1a",
            "762a962e744d4474bb277493860591c2",
            "dc8038e86c154439a956f2a2c50a3bb5",
            "f81807cae71a4691aa84d15e14a691f9",
            "6455c614cd4f4b32a9cd7cbabbb70de5",
            "96c6002fad0149a5b09f9fe3d1fe0a00",
            "4324bed31b0247a382efd3e7175217e3",
            "e1a5bd9aa0a24d159eceedaf04709976",
            "f14e5a6a4acd4420bd615c3b6abceade",
            "3f6d6ecfcffd4949bea209587914d6f9"
          ]
        }
      },
      "source": [
        "#===============================================================================\n",
        "# Part where we train using the 5-fold cv\n",
        "#===============================================================================\n",
        "\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "#train and val are indices\n",
        "kf = KFold(n_splits=5, shuffle = True, random_state = 42)\n",
        "\n",
        "batch_size = 16\n",
        "train_loss = []\n",
        "val_loss = []\n",
        "val_accuracy = []\n",
        "X = dataset['data']\n",
        "y1 = dataset['label']\n",
        "y1 = y1.astype(int)\n",
        "\n",
        "for train_index, val_index in kf.split(X):\n",
        "  #Data Preparation\n",
        "  X_train = X[train_index]\n",
        "  X_val = X[val_index]\n",
        "  y1_train = y1[train_index]\n",
        "  y1_val = y1[val_index]\n",
        "  \n",
        "  train_inputs, train_masks = preprocessing_for_bert(X_train)\n",
        "  val_inputs, val_masks = preprocessing_for_bert(X_val)\n",
        "  train_labels = torch.tensor(y1_train.values)\n",
        "  val_labels = torch.tensor(y1_val.values)\n",
        "  \n",
        "  #Data Loader Class\n",
        "  train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "  train_sampler = RandomSampler(train_data)\n",
        "  train_dataloader = DataLoader(train_data, sampler = train_sampler, batch_size = batch_size)\n",
        "\n",
        "  val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "  val_sampler = RandomSampler(val_data)\n",
        "  val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size = batch_size)\n",
        "\n",
        "  #Fine Tune and Evaluation\n",
        "  set_seed(42)    # Set seed for reproducibility\n",
        "  bert_classifier, optimizer, scheduler = initialize_model(epochs=2)\n",
        "  val_loss1, val_accuracy1 = train(bert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)\n",
        "  \n",
        "  val_loss.append(val_loss1)\n",
        "  val_accuracy.append(val_accuracy1)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0314e94eb78f4eb8a2e70a21734cda66",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dc8038e86c154439a956f2a2c50a3bb5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descriâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Start training...\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   1    |   20    |   0.791206   |     -      |     -     |   10.51  \n",
            "   1    |   40    |   0.709401   |     -      |     -     |   9.75   \n",
            "   1    |   60    |   0.590000   |     -      |     -     |   9.83   \n",
            "   1    |   80    |   0.586745   |     -      |     -     |   9.80   \n",
            "   1    |   96    |   0.474932   |     -      |     -     |   7.50   \n",
            "----------------------------------------------------------------------\n",
            "   1    |    -    |   0.638527   |  0.479735  |   68.75   |   51.18  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   2    |   20    |   0.453002   |     -      |     -     |   10.29  \n",
            "   2    |   40    |   0.443626   |     -      |     -     |   9.77   \n",
            "   2    |   60    |   0.377319   |     -      |     -     |   9.85   \n",
            "   2    |   80    |   0.434456   |     -      |     -     |   9.89   \n",
            "   2    |   96    |   0.408572   |     -      |     -     |   7.59   \n",
            "----------------------------------------------------------------------\n",
            "   2    |    -    |   0.424312   |  0.405657  |   71.00   |   51.19  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Start training...\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   1    |   20    |   0.804154   |     -      |     -     |   10.31  \n",
            "   1    |   40    |   0.730936   |     -      |     -     |   9.75   \n",
            "   1    |   60    |   0.699920   |     -      |     -     |   9.79   \n",
            "   1    |   80    |   0.706750   |     -      |     -     |   9.82   \n",
            "   1    |   96    |   0.617840   |     -      |     -     |   7.53   \n",
            "----------------------------------------------------------------------\n",
            "   1    |    -    |   0.716750   |  0.551305  |   72.25   |   50.99  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   2    |   20    |   0.523053   |     -      |     -     |   10.30  \n",
            "   2    |   40    |   0.508774   |     -      |     -     |   9.84   \n",
            "   2    |   60    |   0.442205   |     -      |     -     |   9.88   \n",
            "   2    |   80    |   0.450502   |     -      |     -     |   9.83   \n",
            "   2    |   96    |   0.510585   |     -      |     -     |   7.62   \n",
            "----------------------------------------------------------------------\n",
            "   2    |    -    |   0.486424   |  0.431129  |   74.50   |   51.27  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Start training...\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   1    |   20    |   0.806786   |     -      |     -     |   10.30  \n",
            "   1    |   40    |   0.712689   |     -      |     -     |   9.82   \n",
            "   1    |   60    |   0.617469   |     -      |     -     |   9.81   \n",
            "   1    |   80    |   0.492912   |     -      |     -     |   9.78   \n",
            "   1    |   96    |   0.425162   |     -      |     -     |   7.55   \n",
            "----------------------------------------------------------------------\n",
            "   1    |    -    |   0.620686   |  0.421865  |   72.50   |   51.04  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   2    |   20    |   0.390593   |     -      |     -     |   10.31  \n",
            "   2    |   40    |   0.429652   |     -      |     -     |   9.92   \n",
            "   2    |   60    |   0.391824   |     -      |     -     |   9.82   \n",
            "   2    |   80    |   0.408397   |     -      |     -     |   9.85   \n",
            "   2    |   96    |   0.409443   |     -      |     -     |   7.57   \n",
            "----------------------------------------------------------------------\n",
            "   2    |    -    |   0.405680   |  0.420979  |   71.25   |   51.26  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Start training...\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   1    |   20    |   0.801649   |     -      |     -     |   10.29  \n",
            "   1    |   40    |   0.724669   |     -      |     -     |   9.84   \n",
            "   1    |   60    |   0.652650   |     -      |     -     |   9.78   \n",
            "   1    |   80    |   0.503648   |     -      |     -     |   9.85   \n",
            "   1    |   96    |   0.478166   |     -      |     -     |   7.55   \n",
            "----------------------------------------------------------------------\n",
            "   1    |    -    |   0.640254   |  0.413677  |   76.25   |   51.09  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   2    |   20    |   0.400808   |     -      |     -     |   10.32  \n",
            "   2    |   40    |   0.426632   |     -      |     -     |   9.79   \n",
            "   2    |   60    |   0.408002   |     -      |     -     |   9.83   \n",
            "   2    |   80    |   0.407582   |     -      |     -     |   9.80   \n",
            "   2    |   96    |   0.456761   |     -      |     -     |   7.58   \n",
            "----------------------------------------------------------------------\n",
            "   2    |    -    |   0.418242   |  0.407222  |   72.25   |   51.10  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Start training...\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   1    |   20    |   0.803848   |     -      |     -     |   10.34  \n",
            "   1    |   40    |   0.726956   |     -      |     -     |   9.87   \n",
            "   1    |   60    |   0.636446   |     -      |     -     |   9.84   \n",
            "   1    |   80    |   0.510596   |     -      |     -     |   9.79   \n",
            "   1    |   96    |   0.486493   |     -      |     -     |   7.51   \n",
            "----------------------------------------------------------------------\n",
            "   1    |    -    |   0.640666   |  0.452276  |   71.25   |   51.13  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   2    |   20    |   0.437284   |     -      |     -     |   10.33  \n",
            "   2    |   40    |   0.446308   |     -      |     -     |   9.78   \n",
            "   2    |   60    |   0.372107   |     -      |     -     |   9.88   \n",
            "   2    |   80    |   0.404919   |     -      |     -     |   9.84   \n",
            "   2    |   96    |   0.423205   |     -      |     -     |   7.53   \n",
            "----------------------------------------------------------------------\n",
            "   2    |    -    |   0.416710   |  0.422822  |   75.75   |   51.15  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}