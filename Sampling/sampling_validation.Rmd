---
title: "Untitled"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(ggthemes)
library(countrycode)

theme_set(theme_gdocs())
```

```{r}
sampled_articles <- read_csv("data/sampled_articles_details.csv")
sampling_process <- read_csv("data/sampling_process.csv")
expected_sampling <- read_csv("misc/sampling_amounts.csv")
file_lookup <- read_csv("data/file_lookup.csv")

sampled_with_succeses <- sampled_articles %>%
  left_join(file_lookup, by = c("year", "country", "month"))

actual_sampling.summary <- sampled_articles %>%
  mutate(has_id = !is.na(id)) %>%
  group_by(country, year) %>%
  summarise(actual_articles_sampled = n(),
            total_sampled_with_ids = sum(has_id)) %>%
  ungroup()

expected_sampling.summary <- expected_sampling %>%
  group_by(country, year) %>%
  summarise(total_articles = sum(total_articles),
            expected_articles_sampled = sum(sampled_articles)) %>%
  ungroup()

sampling_comparison <- expected_sampling.summary %>%
  left_join(actual_sampling.summary) %>%
  mutate(total_sampled_without_ids = actual_articles_sampled - total_sampled_with_ids,
         pct_sampled_without_ids = total_sampled_without_ids / actual_articles_sampled,
         acticles_sampled_delta = actual_articles_sampled - expected_articles_sampled,
         pct_of_total_expected_to_sample = expected_articles_sampled / total_articles,
         pct_of_total_actually_sampled = actual_articles_sampled / total_articles,
         pct_of_total_articles_sampled_delta = pct_of_total_actually_sampled - pct_of_total_expected_to_sample)



```

## Looking for bad files

Most of the bad files seem to be coming from the later years, particularly 2019 and 2020, with the exception of AU, which has a few more bad years. 

```{r}
sampled_articles %>%
  filter(is.na(id), country == "AU")
```

```{r}
sampled_articles %>%
  mutate(has_id = !is.na(id)) %>%
  group_by(country, year) %>%
  summarise(total_articles = n(),
            total_with_ids = sum(has_id)) %>%
  mutate(total_without_ids = total_articles - total_with_ids,
         pct_without_ids = total_without_ids / total_articles) %>%
  ungroup() %>%
  group_by(year) %>%
  mutate(max_pct_without_id = scales::percent(max(pct_without_ids), accuracy = 4)) %>%
  ungroup() %>%
  mutate(display_year = glue::glue("{ year } (max = { max_pct_without_id })")) %>%
  ggplot(aes(pct_without_ids, fct_reorder(country, pct_without_ids), fill = country)) +
  geom_col() + 
  facet_wrap(~ display_year, nrow = 1) +
  scale_x_continuous(breaks = scales::pretty_breaks(3), labels = scales::percent_format(accuracy = 2)) +
  labs(title = "Where are the blank articles coming from?",
       x = "Percent of Sampled Articles with no ID value",
       y = "") + 
  theme(legend.position = "none")
```


While it may be the case that there is a constant error rate in the export pipeline, that doesn't seem to be the case. We compare the distribution of all articles to that of our error files, and 2019 and 2020 still remain outliers. There are more errors happening in these years than should be with the previous assumption. There are also very few if any errors in 2010, 2013 and 2014, which is odd. 

```{r}
sampling_comparison %>%
  group_by(year) %>%
  summarise(total_articles = sum(total_articles),
            actual_articles_sampled = sum(actual_articles_sampled), 
            total_sampled_without_ids = sum(total_sampled_with_ids)) %>%
  mutate(`All Articles` = total_articles / sum(total_articles),
         `Sampled Articles with no ID` = total_sampled_without_ids / sum(total_sampled_without_ids)) %>%
  gather(type, pct, `All Articles`, `Sampled Articles with no ID`) %>%
  select(year, type, pct) %>%
  ggplot(aes(year, pct)) + 
  geom_col() + 
  facet_wrap(~ type) + 
  scale_x_continuous(breaks = scales::pretty_breaks()) + 
  scale_y_continuous(labels = scales::percent_format()) + 
  labs(title = "2019 and 2020 are exporting more bad files then every other year.",
       x = "Year", 
       y = "% of articles")
```

## Comparison of Expected and Actual Sampling 

We see a consistent trend that we sample less articles than we calculated should be. 
```{r}
sampling_comparison %>%
  select(`Difference in Count` = acticles_sampled_delta, 
         `Difference in Percent of Total` = pct_of_total_articles_sampled_delta) %>%
  gather(type, delta) %>%
  ggplot(aes(delta)) + 
  geom_histogram(bins = 30) + 
  facet_wrap(~ type, scales = "free") + 
  labs(title = "We are consistently under the expected sampling amount.",
       subtitle = "However, it seems most countries are fine but a few countries are way off.",
       x = "Difference",
       y = "Count",
       caption = "Difference is defined as 'Expected Amount - Actual Amount'. 'Percent of Total' looks at 'Amount (expected or actual) Sampled / Total Articles in Full Data'")

sampling_comparison %>%
  select(country, year, delta = pct_of_total_articles_sampled_delta) %>%
  ggplot(aes(delta, fct_rev(fct_reorder(country, delta)), fill = country)) +
  geom_col() + 
  facet_wrap(~ year, nrow = 1) +
  scale_x_continuous(breaks = scales::pretty_breaks(2), labels = scales::percent_format()) +
  labs(title = "We are under the expected sampling amounts mostly from 2010-2015.",
       subtitle = "Outliers here are JM and TZ, which are largely under the amount every year.",
       x = "Difference in %",
       y = "") + 
  theme(legend.position = "none")


countries.peaceful <- c("New Zealand", "Canada", "Ireland", "Australia", "United Kingdom", "Singapore")
countries.non_peaceful <- c("Kenya", "Zimbabwe", "Bangladesh", "Pakistan", "Nigeria", "Tanzania")

get_society_cat <- Vectorize(function(country) {
  if (country %in% countries.peaceful) {
    return("Peaceful")
  } else if (country %in% countries.non_peaceful) {
    return("Non-Peaceful")
  } else {
    return("Neutral")
  }
})

sampling_comparison %>%
  group_by(country) %>%
  summarise(total_articles = sum(total_articles),
            expected_articles_sampled = sum(expected_articles_sampled),
            actual_articles_sampled = sum(actual_articles_sampled)) %>%
  mutate(pct_of_total_expected_to_sample = expected_articles_sampled / total_articles,
         pct_of_total_actually_sampled = actual_articles_sampled / total_articles,
         pct_delta = pct_of_total_actually_sampled - pct_of_total_expected_to_sample,
         country_name = countrycode(country, "iso2c", "country.name"),
         society_type = get_society_cat(country_name),
         society_type = factor(society_type, c("Peaceful", "Neutral","Non-Peaceful"))) %>%
  ggplot(aes(total_articles, pct_delta, color = society_type)) + 
  geom_point() + 
  geom_text(aes(label = country), vjust = 1, hjust = 1, check_overlap = TRUE) + 
  scale_x_log10(labels = scales::comma_format()) + 
  scale_y_continuous(breaks = scales::pretty_breaks(2), labels = scales::percent_format()) + 
  scale_color_manual(values = c("#6db7f7", "#36CB8A", "#FF5A59")) + 
  labs(title = "Countries with less articles are more likely to be more undersampled",
       subtitle = "Again we see JM and TZ as outliers",
       x = "Total Articles (from source files)",
       y = "Differnece in %Expected and %Sampled",
       color = "Society Type") + 
  theme(legend.position = "top",
        legend.direction = "horizontal")

```

```{r}
sampling_comparison %>%
  group_by(year) %>%
  summarise(expected = sum(expected_articles_sampled),
            actual = sum(actual_articles_sampled)) %>%
  gather(type, n, -year) %>%
  ggplot(aes(year, n)) +
  geom_col() + 
  facet_wrap(~ type) + 
  scale_x_continuous(breaks = scales::pretty_breaks()) +
  scale_y_continuous(breaks = scales::pretty_breaks(), labels = scales::comma_format())

sampling_comparison %>%
  summarise(expected = sum(expected_articles_sampled),
            actual = sum(actual_articles_sampled)) %>%
  gather(type, n) %>%
  knitr::kable()
```

```{r}
actual_sampling.summary <- sampled_articles %>%
  mutate(has_id = !is.na(id)) %>%
  select(country, publisher = article_publisher, year) %>%
  group_by(country, publisher, year) %>%
  summarise(actual = n()) %>%
  ungroup()

expected_sampling.summary <- expected_sampling %>%
  group_by(country, publisher, year) %>%
  summarise(total_articles = sum(total_articles),
            expected = sum(sampled_articles)) %>%
  ungroup()

sampling_process.summary <- sampling_process %>%
  count(country, year, publisher, 
        valid = valid_articles_found, 
        empty = empty_articles_found, 
        tot_found = total_articles_found) %>%
  select(-n)

sampling_comparison <- expected_sampling.summary %>%
  left_join(actual_sampling.summary, by = c("country", "publisher", "year")) %>%
  left_join(sampling_process.summary, by = c("country", "publisher", "year")) %>%
  mutate(diff = actual - expected)

sampling_comparison %>%
  filter(diff < 0) %>%
  group_by(country) %>%
  summarise(tot_diff = sum(diff))

sampling_comparison %>%
  filter(country %in% c("TZ", "JM"), diff < 0) %>%
  mutate(tot_not_found = total_articles - tot_found)
```

